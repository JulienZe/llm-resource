# llm-resource

LLM全栈优质资源汇总

> 非常欢迎大家也参与到进来，收集更多优质大模型相关资源。


## LLM 算法

### Transformer

原理：
- [Transformer模型详解（图解最完整版](https://zhuanlan.zhihu.com/p/338817680)
- [OpenAI ChatGPT（一）：十分钟读懂 Transformer](https://zhuanlan.zhihu.com/p/600773858)
- [Transformer的结构是什么样的？各个子模块各有什么作用？](https://blog.csdn.net/m0_54929869/article/details/118881804)


源码：

- [OpenAI ChatGPT（一）：Tensorflow实现Transformer](https://zhuanlan.zhihu.com/p/603243890)
- [OpenAI ChatGPT（一）：十分钟读懂 Transformer](https://zhuanlan.zhihu.com/p/600773858)
- [GPT （一）transformer原理和代码详解](https://zhuanlan.zhihu.com/p/632880248)
- [Transformer源码详解（Pytorch版本）](https://zhuanlan.zhihu.com/p/398039366)
- [搞懂Transformer结构，看这篇PyTorch实现就够了](https://zhuanlan.zhihu.com/p/339207092)

### GPT

### GLM 

原理：
- [预训练语言模型：GLM](https://zhuanlan.zhihu.com/p/641499380)


### LLaMA




## LLM 训练


## LLM 推理


## LLM 压缩


- [模型转换、模型压缩、模型加速工具汇总](https://blog.csdn.net/WZZ18191171661/article/details/99700992)

### LLM 量化


### LLM 剪枝


### LLM 蒸馏



## LLM 应用


## AI框架

### PyTorch
- [PyTorch 源码解读系列](https://zhuanlan.zhihu.com/p/328674159) @ OpenMMLab 团队
- [[源码解析] PyTorch 分布式](https://juejin.cn/post/7026144707591815175) @ 罗西的思考

### DeepSpeed


### Megatron-LM

- []()

### Megatron-DeepSpeed





## 综合

- [通向AGI之路：大型语言模型（LLM）技术精要](https://zhuanlan.zhihu.com/p/597586623)
- [大语言模型的涌现能力：现象与解释](https://zhuanlan.zhihu.com/p/621438653)





